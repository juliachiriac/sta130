{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5913d5da",
   "metadata": {},
   "source": [
    "# Pre-lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ff585c",
   "metadata": {},
   "source": [
    "# 1. Explain succinctly in your own words (but working with a ChatBot if needed)...\n",
    "\n",
    "### a) The difference between Simple Linear Regression and Multiple Linear Regression; and the benefit the latter provides over the former\n",
    "\n",
    "### b) the difference between using a continuous variable and an indicator variable in Simple Linear Regression; and these two linear forms\n",
    "\n",
    "### c) the change that happens in the behavior of the model (i.e., the expected nature of the data it models) when a single indicator variable is introduced alongside a continuous variable to create a Multiple Linear Regression; and these two linear forms (i.e., the Simple Linear Regression versus the Multiple Linear Regression)\n",
    "\n",
    "### d) the effect of adding an interaction between a continuous and an indicator variable in Multiple Linear Regression models; and this linear form\n",
    "\n",
    "### e) the behavior of a Multiple Linear Regression model (i.e., the expected nature of the data it models) based only on indicator variables derived from a non-binary categorical variable; this linear form; and the necessarily resulting binary variable encodings it utilizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a52d12a",
   "metadata": {},
   "source": [
    "a) The difference between Simple Linear Regression (SLR) and Multiple Linear Regression (MLR) is that they have a different number of predictors used for each. SLR involves only one independent variable to predict the dependent variable. Whereas the MLR model involves two or more independent variables to predict the dependent variable. The model tries to find a hyperplane that best fits the data points in a higher-dimensional space. The MLR model has an improved prediction accuracy because it considers additional predictors, which can explain more variation in the dependent variable. Additionally, there is reduced bias because it includes relevant predictors which can reduce omitted variable bias, making the model’s predictions more reliable.\n",
    "\n",
    "b) A continuous variable in SLR takes on a range of numerical values, representing a variable that can be measured on a continuum, therefore when used as the predictor in SLR, the model estimates how each one-unit increase in this continuous predictor changes the outcome variable. Whereas an indicator variable is a binary (0 or 1) variable used to represent categories, such as group membership. When it is used as a predictor, it allows the model to estimate a different mean outcome for each category it represents. Therefore continuous variables are used when you expect a gradual, proportional change, while indicator variables are used to distinguish between discrete groups.\n",
    "\n",
    "c) When a single indicator variable is introduced alongside a continuous variable in a MLR model, the model gains the ability to capture interactions or shifts in the relationship between the continuous variable and the outcome based on the indicator. This setup allows the model to produce separate baseline outcomes (intercepts) for each group, effectively modeling two parallel lines—one for each group defined by the indicator variable—with a shared slope based on the continuous variable. However, a SLR with only a continuous predictor assumes a single trend across all data points, missing any group-level differences. The inclusion of the indicator variable in MLR thus enables the model to represent both the overall effect of the continuous variable and the specific shift in baseline outcomes for each group, making it more flexible and accurate when subgroup differences are present in the data.\n",
    "\n",
    "d) Adding an interaction between a continuous variable and an indicator variable in a Multiple Linear Regression model allows each group to have its own slope, meaning the effect of the continuous predictor differs by group. This adjustment creates non-parallel lines, capturing variations in the continuous predictor’s impact across groups, which a standard MLR without interaction cannot model.\n",
    "\n",
    "e) A MLR model based solely on indicator variables derived from a non-binary categorical variable captures differences in the outcome across multiple distinct groups, without assuming any continuous trend. This setup uses dummy (one-hot) encoding, where each category is represented by a separate binary (0 or 1) indicator variable, except for one baseline category. Each indicator variable thus corresponds to a specific category, and the model estimates the change in the outcome for each category relative to this baseline. By using binary encodings, the MLR model provides an intercept-adjusted outcome prediction for each category, reflecting discrete, categorical differences rather than any continuous trend. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2561b06d",
   "metadata": {},
   "source": [
    "### CHAT SUMMARY THUS FAR \n",
    "    \n",
    "We discussed differences in linear regression models, focusing on the roles of continuous and indicator variables in Simple and Multiple Linear Regression (MLR). Simple Linear Regression uses a single predictor, while MLR can include multiple predictors, offering greater flexibility. Adding an indicator variable to MLR allows the model to differentiate intercepts across groups, and including an interaction term enables each group to have its own slope, reflecting differences in the continuous predictor’s effect across groups.\n",
    "\n",
    "For models with only indicator variables derived from a non-binary categorical variable, MLR uses one-hot encoding to represent each category. This allows the model to capture unique group effects relative to a baseline, without implying a continuous trend. This approach models the discrete differences across categories rather than any linear progression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c0f799",
   "metadata": {},
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198e139e",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f3b2dc",
   "metadata": {},
   "source": [
    "# 4. Explain the apparent contradiction between the factual statements regarding the fit below that \"the model only explains 17.6% of the variability in the data\" \n",
    "# while at the same time \"many of the coefficients are larger than 10 while having strong or very strong evidence against the null hypothesis of 'no effect'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "407ce9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>VenusaurMega Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>123</td>\n",
       "      <td>122</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Charmander</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>719</td>\n",
       "      <td>Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>719</td>\n",
       "      <td>DiancieMega Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Confined</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Unbound</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Dark</td>\n",
       "      <td>80</td>\n",
       "      <td>160</td>\n",
       "      <td>60</td>\n",
       "      <td>170</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>721</td>\n",
       "      <td>Volcanion</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Water</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>120</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #                   Name   Type 1  Type 2  HP  Attack  Defense  \\\n",
       "0      1              Bulbasaur    Grass  Poison  45      49       49   \n",
       "1      2                Ivysaur    Grass  Poison  60      62       63   \n",
       "2      3               Venusaur    Grass  Poison  80      82       83   \n",
       "3      3  VenusaurMega Venusaur    Grass  Poison  80     100      123   \n",
       "4      4             Charmander     Fire     NaN  39      52       43   \n",
       "..   ...                    ...      ...     ...  ..     ...      ...   \n",
       "795  719                Diancie     Rock   Fairy  50     100      150   \n",
       "796  719    DiancieMega Diancie     Rock   Fairy  50     160      110   \n",
       "797  720    HoopaHoopa Confined  Psychic   Ghost  80     110       60   \n",
       "798  720     HoopaHoopa Unbound  Psychic    Dark  80     160       60   \n",
       "799  721              Volcanion     Fire   Water  80     110      120   \n",
       "\n",
       "     Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "0         65       65     45           1      False  \n",
       "1         80       80     60           1      False  \n",
       "2        100      100     80           1      False  \n",
       "3        122      120     80           1      False  \n",
       "4         60       50     65           1      False  \n",
       "..       ...      ...    ...         ...        ...  \n",
       "795      100      150     50           6       True  \n",
       "796      160      110    110           6       True  \n",
       "797      150      130     70           6       True  \n",
       "798      170      130     80           6       True  \n",
       "799      130       90     70           6       True  \n",
       "\n",
       "[800 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "# fail https://github.com/KeithGalli/pandas/blob/master/pokemon_data.csv\n",
    "pokeaman = pd.read_csv(url) \n",
    "pokeaman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d53102f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 14 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>3.50e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:13:21</td>     <th>  Log-Likelihood:    </th> <td> -3649.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   800</td>      <th>  AIC:               </th> <td>   7323.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   788</td>      <th>  BIC:               </th> <td>   7379.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                 <td></td>                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                       <td>   26.8971</td> <td>    5.246</td> <td>    5.127</td> <td> 0.000</td> <td>   16.599</td> <td>   37.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>              <td>   20.0449</td> <td>    7.821</td> <td>    2.563</td> <td> 0.011</td> <td>    4.692</td> <td>   35.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>              <td>   21.3662</td> <td>    6.998</td> <td>    3.053</td> <td> 0.002</td> <td>    7.629</td> <td>   35.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>              <td>   31.9575</td> <td>    8.235</td> <td>    3.881</td> <td> 0.000</td> <td>   15.793</td> <td>   48.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>              <td>    9.4926</td> <td>    7.883</td> <td>    1.204</td> <td> 0.229</td> <td>   -5.982</td> <td>   24.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>              <td>   22.2693</td> <td>    8.709</td> <td>    2.557</td> <td> 0.011</td> <td>    5.173</td> <td>   39.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                    <td>    0.5634</td> <td>    0.071</td> <td>    7.906</td> <td> 0.000</td> <td>    0.423</td> <td>    0.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.2]</th> <td>   -0.2350</td> <td>    0.101</td> <td>   -2.316</td> <td> 0.021</td> <td>   -0.434</td> <td>   -0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.3]</th> <td>   -0.3067</td> <td>    0.093</td> <td>   -3.300</td> <td> 0.001</td> <td>   -0.489</td> <td>   -0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.4]</th> <td>   -0.3790</td> <td>    0.105</td> <td>   -3.600</td> <td> 0.000</td> <td>   -0.586</td> <td>   -0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.5]</th> <td>   -0.0484</td> <td>    0.108</td> <td>   -0.447</td> <td> 0.655</td> <td>   -0.261</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.6]</th> <td>   -0.3083</td> <td>    0.112</td> <td>   -2.756</td> <td> 0.006</td> <td>   -0.528</td> <td>   -0.089</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>337.229</td> <th>  Durbin-Watson:     </th> <td>   1.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2871.522</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.684</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>11.649</td>  <th>  Cond. No.          </th> <td>1.40e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.4e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                  &        HP        & \\textbf{  R-squared:         } &     0.176   \\\\\n",
       "\\textbf{Model:}                          &       OLS        & \\textbf{  Adj. R-squared:    } &     0.164   \\\\\n",
       "\\textbf{Method:}                         &  Least Squares   & \\textbf{  F-statistic:       } &     15.27   \\\\\n",
       "\\textbf{Date:}                           & Thu, 14 Nov 2024 & \\textbf{  Prob (F-statistic):} &  3.50e-27   \\\\\n",
       "\\textbf{Time:}                           &     21:13:21     & \\textbf{  Log-Likelihood:    } &   -3649.4   \\\\\n",
       "\\textbf{No. Observations:}               &         800      & \\textbf{  AIC:               } &     7323.   \\\\\n",
       "\\textbf{Df Residuals:}                   &         788      & \\textbf{  BIC:               } &     7379.   \\\\\n",
       "\\textbf{Df Model:}                       &          11      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                         & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                       &      26.8971  &        5.246     &     5.127  &         0.000        &       16.599    &       37.195     \\\\\n",
       "\\textbf{C(Generation)[T.2]}              &      20.0449  &        7.821     &     2.563  &         0.011        &        4.692    &       35.398     \\\\\n",
       "\\textbf{C(Generation)[T.3]}              &      21.3662  &        6.998     &     3.053  &         0.002        &        7.629    &       35.103     \\\\\n",
       "\\textbf{C(Generation)[T.4]}              &      31.9575  &        8.235     &     3.881  &         0.000        &       15.793    &       48.122     \\\\\n",
       "\\textbf{C(Generation)[T.5]}              &       9.4926  &        7.883     &     1.204  &         0.229        &       -5.982    &       24.968     \\\\\n",
       "\\textbf{C(Generation)[T.6]}              &      22.2693  &        8.709     &     2.557  &         0.011        &        5.173    &       39.366     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                    &       0.5634  &        0.071     &     7.906  &         0.000        &        0.423    &        0.703     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.2]} &      -0.2350  &        0.101     &    -2.316  &         0.021        &       -0.434    &       -0.036     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.3]} &      -0.3067  &        0.093     &    -3.300  &         0.001        &       -0.489    &       -0.124     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.4]} &      -0.3790  &        0.105     &    -3.600  &         0.000        &       -0.586    &       -0.172     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.5]} &      -0.0484  &        0.108     &    -0.447  &         0.655        &       -0.261    &        0.164     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.6]} &      -0.3083  &        0.112     &    -2.756  &         0.006        &       -0.528    &       -0.089     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 337.229 & \\textbf{  Durbin-Watson:     } &    1.505  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2871.522  \\\\\n",
       "\\textbf{Skew:}          &   1.684 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  11.649 & \\textbf{  Cond. No.          } & 1.40e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.4e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.176\n",
       "Model:                            OLS   Adj. R-squared:                  0.164\n",
       "Method:                 Least Squares   F-statistic:                     15.27\n",
       "Date:                Thu, 14 Nov 2024   Prob (F-statistic):           3.50e-27\n",
       "Time:                        21:13:21   Log-Likelihood:                -3649.4\n",
       "No. Observations:                 800   AIC:                             7323.\n",
       "Df Residuals:                     788   BIC:                             7379.\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================================\n",
       "                                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------------\n",
       "Intercept                          26.8971      5.246      5.127      0.000      16.599      37.195\n",
       "C(Generation)[T.2]                 20.0449      7.821      2.563      0.011       4.692      35.398\n",
       "C(Generation)[T.3]                 21.3662      6.998      3.053      0.002       7.629      35.103\n",
       "C(Generation)[T.4]                 31.9575      8.235      3.881      0.000      15.793      48.122\n",
       "C(Generation)[T.5]                  9.4926      7.883      1.204      0.229      -5.982      24.968\n",
       "C(Generation)[T.6]                 22.2693      8.709      2.557      0.011       5.173      39.366\n",
       "Q(\"Sp. Def\")                        0.5634      0.071      7.906      0.000       0.423       0.703\n",
       "Q(\"Sp. Def\"):C(Generation)[T.2]    -0.2350      0.101     -2.316      0.021      -0.434      -0.036\n",
       "Q(\"Sp. Def\"):C(Generation)[T.3]    -0.3067      0.093     -3.300      0.001      -0.489      -0.124\n",
       "Q(\"Sp. Def\"):C(Generation)[T.4]    -0.3790      0.105     -3.600      0.000      -0.586      -0.172\n",
       "Q(\"Sp. Def\"):C(Generation)[T.5]    -0.0484      0.108     -0.447      0.655      -0.261       0.164\n",
       "Q(\"Sp. Def\"):C(Generation)[T.6]    -0.3083      0.112     -2.756      0.006      -0.528      -0.089\n",
       "==============================================================================\n",
       "Omnibus:                      337.229   Durbin-Watson:                   1.505\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2871.522\n",
       "Skew:                           1.684   Prob(JB):                         0.00\n",
       "Kurtosis:                      11.649   Cond. No.                     1.40e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.4e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model1_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation) + Q(\"Sp. Def\"):C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") * C(Generation)', data=pokeaman)\n",
    "\n",
    "model2_fit = model2_spec.fit()\n",
    "model2_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f97107b",
   "metadata": {},
   "source": [
    "The apparent contradiction between a low R^2 value (indicating the model explains only 17.6% of the variance in HP) and strong, statistically significant coefficients can be understood by recognizing that these metrics address different aspects of the model. R^2 measures the overall explanatory power and an R^2 of 17.6% (considered low) suggests that a large portion of the variability in HP remains unexplained by the model, implying that other factors not included in the model may influence HP. Meanwhile, low p-values indicate that the predictors included are statistically significant, meaning they have a meaningful effect on HP despite not fully explaining its variability. Using \"Generation\" as a categorical predictor with interactions helps capture differences between groups but does not necessarily raise R^2, as it highlights group distinctions rather than fully explaining HP. Thus, R^2 assesses the model’s fit, while p-values and coefficient sizes reflect each predictor's impact. Together, these metrics provide complementary insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b3848f",
   "metadata": {},
   "source": [
    "### Chat summary and link for everything until this point: https://chatgpt.com/share/6734f6f1-30e8-8003-9aa7-a3956d2b3f5f\n",
    "\n",
    "We discussed interpreting Multiple Linear Regression (MLR) metrics, particularly how to reconcile a low R^2 value with statistically significant coefficients. We noted that R^2 measures the overall explanatory power of the model, with a low R^2 indicating that much variability in the outcome variable remains unexplained. In contrast, p-values test the significance of each predictor, and low p-values indicate that the predictors are impactful within the model despite not fully explaining the outcome's variability.\n",
    "\n",
    "We also examined how adding an indicator variable, especially with interaction terms, can capture differences between groups. Treating \"Generation\" as a categorical variable helps model group distinctions without necessarily increasing R^2. Finally, I simplified a detailed paragraph for clarity, condensing redundant explanations and emphasizing how R^2 and p-values offer complementary insights on model fit and predictor impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2453711f",
   "metadata": {},
   "source": [
    "# post lecture "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62915920",
   "metadata": {},
   "source": [
    "# 5. Discuss the following (five cells of) code and results with a ChatBot and based on the understanding you arrive at in this conversation explain what the following (five cells of) are illustrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79bfa564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>338</td>\n",
       "      <td>Solrock</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>70</td>\n",
       "      <td>95</td>\n",
       "      <td>85</td>\n",
       "      <td>55</td>\n",
       "      <td>65</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Charizard</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Flying</td>\n",
       "      <td>78</td>\n",
       "      <td>84</td>\n",
       "      <td>78</td>\n",
       "      <td>109</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>224</td>\n",
       "      <td>Octillery</td>\n",
       "      <td>Water</td>\n",
       "      <td>None</td>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>75</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>600</td>\n",
       "      <td>Klang</td>\n",
       "      <td>Steel</td>\n",
       "      <td>None</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>95</td>\n",
       "      <td>70</td>\n",
       "      <td>85</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>265</td>\n",
       "      <td>Wurmple</td>\n",
       "      <td>Bug</td>\n",
       "      <td>None</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>471</td>\n",
       "      <td>Glaceon</td>\n",
       "      <td>Ice</td>\n",
       "      <td>None</td>\n",
       "      <td>65</td>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>130</td>\n",
       "      <td>95</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>225</td>\n",
       "      <td>Delibird</td>\n",
       "      <td>Ice</td>\n",
       "      <td>Flying</td>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Confined</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>109</td>\n",
       "      <td>Koffing</td>\n",
       "      <td>Poison</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>65</td>\n",
       "      <td>95</td>\n",
       "      <td>60</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>373</td>\n",
       "      <td>SalamenceMega Salamence</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>Flying</td>\n",
       "      <td>95</td>\n",
       "      <td>145</td>\n",
       "      <td>130</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #                     Name   Type 1   Type 2  HP  Attack  Defense  \\\n",
       "370  338                  Solrock     Rock  Psychic  70      95       85   \n",
       "6      6                Charizard     Fire   Flying  78      84       78   \n",
       "242  224                Octillery    Water     None  75     105       75   \n",
       "661  600                    Klang    Steel     None  60      80       95   \n",
       "288  265                  Wurmple      Bug     None  45      45       35   \n",
       "..   ...                      ...      ...      ...  ..     ...      ...   \n",
       "522  471                  Glaceon      Ice     None  65      60      110   \n",
       "243  225                 Delibird      Ice   Flying  45      55       45   \n",
       "797  720      HoopaHoopa Confined  Psychic    Ghost  80     110       60   \n",
       "117  109                  Koffing   Poison     None  40      65       95   \n",
       "409  373  SalamenceMega Salamence   Dragon   Flying  95     145      130   \n",
       "\n",
       "     Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "370       55       65     70           3      False  \n",
       "6        109       85    100           1      False  \n",
       "242      105       75     45           2      False  \n",
       "661       70       85     50           5      False  \n",
       "288       20       30     20           3      False  \n",
       "..       ...      ...    ...         ...        ...  \n",
       "522      130       95     65           4      False  \n",
       "243       65       45     75           2      False  \n",
       "797      150      130     70           6       True  \n",
       "117       60       45     35           1      False  \n",
       "409      120       90    120           3      False  \n",
       "\n",
       "[400 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
    "\n",
    "# Replace \"NaN\" (in the \"Type 2\" column with \"None\")\n",
    "pokeaman.fillna('None', inplace=True)\n",
    "\n",
    "np.random.seed(130)\n",
    "pokeaman_train,pokeaman_test = \\\n",
    "  train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
    "pokeaman_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a138eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 14 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>1.66e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:13:27</td>     <th>  Log-Likelihood:    </th> <td> -1832.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3671.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   3683.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   42.5882</td> <td>    3.580</td> <td>   11.897</td> <td> 0.000</td> <td>   35.551</td> <td>   49.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>    <td>    0.2472</td> <td>    0.041</td> <td>    6.051</td> <td> 0.000</td> <td>    0.167</td> <td>    0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>   <td>    0.1001</td> <td>    0.045</td> <td>    2.201</td> <td> 0.028</td> <td>    0.011</td> <td>    0.190</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>284.299</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5870.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.720</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>20.963</td>  <th>  Cond. No.          </th> <td>    343.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        HP        & \\textbf{  R-squared:         } &     0.148   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.143   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     34.40   \\\\\n",
       "\\textbf{Date:}             & Thu, 14 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.66e-14   \\\\\n",
       "\\textbf{Time:}             &     21:13:27     & \\textbf{  Log-Likelihood:    } &   -1832.6   \\\\\n",
       "\\textbf{No. Observations:} &         400      & \\textbf{  AIC:               } &     3671.   \\\\\n",
       "\\textbf{Df Residuals:}     &         397      & \\textbf{  BIC:               } &     3683.   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      42.5882  &        3.580     &    11.897  &         0.000        &       35.551    &       49.626     \\\\\n",
       "\\textbf{Attack}    &       0.2472  &        0.041     &     6.051  &         0.000        &        0.167    &        0.327     \\\\\n",
       "\\textbf{Defense}   &       0.1001  &        0.045     &     2.201  &         0.028        &        0.011    &        0.190     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 284.299 & \\textbf{  Durbin-Watson:     } &    2.006  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5870.841  \\\\\n",
       "\\textbf{Skew:}          &   2.720 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  20.963 & \\textbf{  Cond. No.          } &     343.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.148\n",
       "Model:                            OLS   Adj. R-squared:                  0.143\n",
       "Method:                 Least Squares   F-statistic:                     34.40\n",
       "Date:                Thu, 14 Nov 2024   Prob (F-statistic):           1.66e-14\n",
       "Time:                        21:13:27   Log-Likelihood:                -1832.6\n",
       "No. Observations:                 400   AIC:                             3671.\n",
       "Df Residuals:                     397   BIC:                             3683.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     42.5882      3.580     11.897      0.000      35.551      49.626\n",
       "Attack         0.2472      0.041      6.051      0.000       0.167       0.327\n",
       "Defense        0.1001      0.045      2.201      0.028       0.011       0.190\n",
       "==============================================================================\n",
       "Omnibus:                      284.299   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5870.841\n",
       "Skew:                           2.720   Prob(JB):                         0.00\n",
       "Kurtosis:                      20.963   Cond. No.                         343.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_spec3 = smf.ols(formula='HP ~ Attack + Defense', \n",
    "                      data=pokeaman_train)\n",
    "model3_fit = model_spec3.fit()\n",
    "model3_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7abe34b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.14771558304519894\n",
      "'Out of sample' R-squared: 0.21208501873920738\n"
     ]
    }
   ],
   "source": [
    "yhat_model3 = model3_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model3_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model3)[0,1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef2515b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.764</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 14 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>4.23e-21</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:13:28</td>     <th>  Log-Likelihood:    </th> <td> -1738.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3603.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   337</td>      <th>  BIC:               </th> <td>   3855.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    62</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                                  <td></td>                                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                                        <td>  521.5715</td> <td>  130.273</td> <td>    4.004</td> <td> 0.000</td> <td>  265.322</td> <td>  777.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]</th>                                                <td>   -6.1179</td> <td>    2.846</td> <td>   -2.150</td> <td> 0.032</td> <td>  -11.716</td> <td>   -0.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                                                           <td>   -8.1938</td> <td>    2.329</td> <td>   -3.518</td> <td> 0.000</td> <td>  -12.775</td> <td>   -3.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]</th>                                         <td>-1224.9610</td> <td>  545.105</td> <td>   -2.247</td> <td> 0.025</td> <td>-2297.199</td> <td> -152.723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>                                                          <td>   -6.1989</td> <td>    2.174</td> <td>   -2.851</td> <td> 0.005</td> <td>  -10.475</td> <td>   -1.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]</th>                                        <td> -102.4030</td> <td>   96.565</td> <td>   -1.060</td> <td> 0.290</td> <td> -292.350</td> <td>   87.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense</th>                                                   <td>    0.0985</td> <td>    0.033</td> <td>    2.982</td> <td> 0.003</td> <td>    0.034</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]</th>                                 <td>   14.6361</td> <td>    6.267</td> <td>    2.336</td> <td> 0.020</td> <td>    2.310</td> <td>   26.963</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                                                            <td>   -7.2261</td> <td>    2.178</td> <td>   -3.318</td> <td> 0.001</td> <td>  -11.511</td> <td>   -2.942</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]</th>                                          <td>  704.8798</td> <td>  337.855</td> <td>    2.086</td> <td> 0.038</td> <td>   40.309</td> <td> 1369.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed</th>                                                     <td>    0.1264</td> <td>    0.038</td> <td>    3.351</td> <td> 0.001</td> <td>    0.052</td> <td>    0.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]</th>                                   <td>    5.8648</td> <td>    2.692</td> <td>    2.179</td> <td> 0.030</td> <td>    0.570</td> <td>   11.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed</th>                                                    <td>    0.1026</td> <td>    0.039</td> <td>    2.634</td> <td> 0.009</td> <td>    0.026</td> <td>    0.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]</th>                                  <td>   -6.9266</td> <td>    3.465</td> <td>   -1.999</td> <td> 0.046</td> <td>  -13.742</td> <td>   -0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed</th>                                             <td>   -0.0016</td> <td>    0.001</td> <td>   -2.837</td> <td> 0.005</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]</th>                           <td>   -0.0743</td> <td>    0.030</td> <td>   -2.477</td> <td> 0.014</td> <td>   -0.133</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                                                     <td>   -5.3982</td> <td>    1.938</td> <td>   -2.785</td> <td> 0.006</td> <td>   -9.211</td> <td>   -1.586</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Def\")</th>                                   <td> -282.2496</td> <td>  126.835</td> <td>   -2.225</td> <td> 0.027</td> <td> -531.738</td> <td>  -32.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\")</th>                                              <td>    0.1094</td> <td>    0.034</td> <td>    3.233</td> <td> 0.001</td> <td>    0.043</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Def\")</th>                            <td>   12.6503</td> <td>    5.851</td> <td>    2.162</td> <td> 0.031</td> <td>    1.141</td> <td>   24.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Def\")</th>                                             <td>    0.0628</td> <td>    0.028</td> <td>    2.247</td> <td> 0.025</td> <td>    0.008</td> <td>    0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Def\")</th>                           <td>    3.3949</td> <td>    1.783</td> <td>    1.904</td> <td> 0.058</td> <td>   -0.112</td> <td>    6.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Def\")</th>                                      <td>   -0.0012</td> <td>    0.000</td> <td>   -2.730</td> <td> 0.007</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")</th>                    <td>   -0.1456</td> <td>    0.065</td> <td>   -2.253</td> <td> 0.025</td> <td>   -0.273</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\")</th>                                               <td>    0.0624</td> <td>    0.031</td> <td>    2.027</td> <td> 0.043</td> <td>    0.002</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                             <td>   -3.2219</td> <td>    1.983</td> <td>   -1.625</td> <td> 0.105</td> <td>   -7.122</td> <td>    0.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\")</th>                                        <td>   -0.0014</td> <td>    0.001</td> <td>   -2.732</td> <td> 0.007</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                      <td>   -0.0695</td> <td>    0.033</td> <td>   -2.100</td> <td> 0.036</td> <td>   -0.135</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Def\")</th>                                       <td>   -0.0008</td> <td>    0.000</td> <td>   -1.743</td> <td> 0.082</td> <td>   -0.002</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                     <td>    0.0334</td> <td>    0.021</td> <td>    1.569</td> <td> 0.117</td> <td>   -0.008</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Def\")</th>                                <td> 1.629e-05</td> <td> 6.92e-06</td> <td>    2.355</td> <td> 0.019</td> <td> 2.68e-06</td> <td> 2.99e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>              <td>    0.0008</td> <td>    0.000</td> <td>    2.433</td> <td> 0.015</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>                                                     <td>   -8.3636</td> <td>    2.346</td> <td>   -3.565</td> <td> 0.000</td> <td>  -12.978</td> <td>   -3.749</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Atk\")</th>                                   <td>  850.5436</td> <td>  385.064</td> <td>    2.209</td> <td> 0.028</td> <td>   93.112</td> <td> 1607.975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Atk\")</th>                                              <td>    0.1388</td> <td>    0.040</td> <td>    3.500</td> <td> 0.001</td> <td>    0.061</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Atk\")</th>                            <td>    2.1809</td> <td>    1.136</td> <td>    1.920</td> <td> 0.056</td> <td>   -0.054</td> <td>    4.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Atk\")</th>                                             <td>    0.0831</td> <td>    0.038</td> <td>    2.162</td> <td> 0.031</td> <td>    0.007</td> <td>    0.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Atk\")</th>                           <td>   -7.3121</td> <td>    3.376</td> <td>   -2.166</td> <td> 0.031</td> <td>  -13.953</td> <td>   -0.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Atk\")</th>                                      <td>   -0.0014</td> <td>    0.001</td> <td>   -2.480</td> <td> 0.014</td> <td>   -0.003</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")</th>                    <td>   -0.0434</td> <td>    0.022</td> <td>   -2.010</td> <td> 0.045</td> <td>   -0.086</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Atk\")</th>                                               <td>    0.1011</td> <td>    0.035</td> <td>    2.872</td> <td> 0.004</td> <td>    0.032</td> <td>    0.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                             <td>  -12.6343</td> <td>    5.613</td> <td>   -2.251</td> <td> 0.025</td> <td>  -23.674</td> <td>   -1.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Atk\")</th>                                        <td>   -0.0018</td> <td>    0.001</td> <td>   -3.102</td> <td> 0.002</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                      <td>    0.0151</td> <td>    0.009</td> <td>    1.609</td> <td> 0.109</td> <td>   -0.003</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Atk\")</th>                                       <td>   -0.0012</td> <td>    0.001</td> <td>   -1.860</td> <td> 0.064</td> <td>   -0.002</td> <td> 6.62e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                     <td>    0.1210</td> <td>    0.054</td> <td>    2.260</td> <td> 0.024</td> <td>    0.016</td> <td>    0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Atk\")</th>                                <td> 2.125e-05</td> <td>  9.1e-06</td> <td>    2.334</td> <td> 0.020</td> <td> 3.34e-06</td> <td> 3.92e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>              <td> 6.438e-06</td> <td> 7.69e-05</td> <td>    0.084</td> <td> 0.933</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                        <td>    0.1265</td> <td>    0.033</td> <td>    3.821</td> <td> 0.000</td> <td>    0.061</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                      <td>   -5.0544</td> <td>    2.506</td> <td>   -2.017</td> <td> 0.044</td> <td>   -9.983</td> <td>   -0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                 <td>   -0.0021</td> <td>    0.001</td> <td>   -3.606</td> <td> 0.000</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>               <td>   -0.0346</td> <td>    0.017</td> <td>   -1.992</td> <td> 0.047</td> <td>   -0.069</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                <td>   -0.0012</td> <td>    0.000</td> <td>   -2.406</td> <td> 0.017</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>              <td>    0.0446</td> <td>    0.025</td> <td>    1.794</td> <td> 0.074</td> <td>   -0.004</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                         <td> 1.973e-05</td> <td> 7.28e-06</td> <td>    2.710</td> <td> 0.007</td> <td> 5.41e-06</td> <td>  3.4e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>       <td>    0.0005</td> <td>    0.000</td> <td>    1.957</td> <td> 0.051</td> <td>-2.56e-06</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                  <td>   -0.0013</td> <td>    0.000</td> <td>   -2.740</td> <td> 0.006</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                <td>    0.0841</td> <td>    0.040</td> <td>    2.125</td> <td> 0.034</td> <td>    0.006</td> <td>    0.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                           <td> 2.379e-05</td> <td> 7.85e-06</td> <td>    3.030</td> <td> 0.003</td> <td> 8.34e-06</td> <td> 3.92e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>         <td> 2.864e-05</td> <td> 7.73e-05</td> <td>    0.370</td> <td> 0.711</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                          <td> 1.284e-05</td> <td> 7.46e-06</td> <td>    1.721</td> <td> 0.086</td> <td>-1.83e-06</td> <td> 2.75e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>        <td>   -0.0008</td> <td>    0.000</td> <td>   -2.085</td> <td> 0.038</td> <td>   -0.002</td> <td>-4.68e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                   <td> -2.53e-07</td> <td>  1.1e-07</td> <td>   -2.292</td> <td> 0.023</td> <td> -4.7e-07</td> <td>-3.59e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th> <td>-1.425e-06</td> <td> 1.14e-06</td> <td>   -1.249</td> <td> 0.212</td> <td>-3.67e-06</td> <td> 8.19e-07</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.20e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.2e+16. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                                                   &        HP        & \\textbf{  R-squared:         } &     0.467   \\\\\n",
       "\\textbf{Model:}                                                           &       OLS        & \\textbf{  Adj. R-squared:    } &     0.369   \\\\\n",
       "\\textbf{Method:}                                                          &  Least Squares   & \\textbf{  F-statistic:       } &     4.764   \\\\\n",
       "\\textbf{Date:}                                                            & Thu, 14 Nov 2024 & \\textbf{  Prob (F-statistic):} &  4.23e-21   \\\\\n",
       "\\textbf{Time:}                                                            &     21:13:28     & \\textbf{  Log-Likelihood:    } &   -1738.6   \\\\\n",
       "\\textbf{No. Observations:}                                                &         400      & \\textbf{  AIC:               } &     3603.   \\\\\n",
       "\\textbf{Df Residuals:}                                                    &         337      & \\textbf{  BIC:               } &     3855.   \\\\\n",
       "\\textbf{Df Model:}                                                        &          62      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                                                 &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                                          & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                                        &     521.5715  &      130.273     &     4.004  &         0.000        &      265.322    &      777.821     \\\\\n",
       "\\textbf{Legendary[T.True]}                                                &      -6.1179  &        2.846     &    -2.150  &         0.032        &      -11.716    &       -0.520     \\\\\n",
       "\\textbf{Attack}                                                           &      -8.1938  &        2.329     &    -3.518  &         0.000        &      -12.775    &       -3.612     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]}                                         &   -1224.9610  &      545.105     &    -2.247  &         0.025        &    -2297.199    &     -152.723     \\\\\n",
       "\\textbf{Defense}                                                          &      -6.1989  &        2.174     &    -2.851  &         0.005        &      -10.475    &       -1.923     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]}                                        &    -102.4030  &       96.565     &    -1.060  &         0.290        &     -292.350    &       87.544     \\\\\n",
       "\\textbf{Attack:Defense}                                                   &       0.0985  &        0.033     &     2.982  &         0.003        &        0.034    &        0.164     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]}                                 &      14.6361  &        6.267     &     2.336  &         0.020        &        2.310    &       26.963     \\\\\n",
       "\\textbf{Speed}                                                            &      -7.2261  &        2.178     &    -3.318  &         0.001        &      -11.511    &       -2.942     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]}                                          &     704.8798  &      337.855     &     2.086  &         0.038        &       40.309    &     1369.450     \\\\\n",
       "\\textbf{Attack:Speed}                                                     &       0.1264  &        0.038     &     3.351  &         0.001        &        0.052    &        0.201     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]}                                   &       5.8648  &        2.692     &     2.179  &         0.030        &        0.570    &       11.160     \\\\\n",
       "\\textbf{Defense:Speed}                                                    &       0.1026  &        0.039     &     2.634  &         0.009        &        0.026    &        0.179     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]}                                  &      -6.9266  &        3.465     &    -1.999  &         0.046        &      -13.742    &       -0.111     \\\\\n",
       "\\textbf{Attack:Defense:Speed}                                             &      -0.0016  &        0.001     &    -2.837  &         0.005        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]}                           &      -0.0743  &        0.030     &    -2.477  &         0.014        &       -0.133    &       -0.015     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                                                     &      -5.3982  &        1.938     &    -2.785  &         0.006        &       -9.211    &       -1.586     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Def\")}                                   &    -282.2496  &      126.835     &    -2.225  &         0.027        &     -531.738    &      -32.761     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\")}                                              &       0.1094  &        0.034     &     3.233  &         0.001        &        0.043    &        0.176     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Def\")}                            &      12.6503  &        5.851     &     2.162  &         0.031        &        1.141    &       24.160     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Def\")}                                             &       0.0628  &        0.028     &     2.247  &         0.025        &        0.008    &        0.118     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Def\")}                           &       3.3949  &        1.783     &     1.904  &         0.058        &       -0.112    &        6.902     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Def\")}                                      &      -0.0012  &        0.000     &    -2.730  &         0.007        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")}                    &      -0.1456  &        0.065     &    -2.253  &         0.025        &       -0.273    &       -0.018     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\")}                                               &       0.0624  &        0.031     &     2.027  &         0.043        &        0.002    &        0.123     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Def\")}                             &      -3.2219  &        1.983     &    -1.625  &         0.105        &       -7.122    &        0.678     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\")}                                        &      -0.0014  &        0.001     &    -2.732  &         0.007        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")}                      &      -0.0695  &        0.033     &    -2.100  &         0.036        &       -0.135    &       -0.004     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Def\")}                                       &      -0.0008  &        0.000     &    -1.743  &         0.082        &       -0.002    &        0.000     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")}                     &       0.0334  &        0.021     &     1.569  &         0.117        &       -0.008    &        0.075     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Def\")}                                &    1.629e-05  &     6.92e-06     &     2.355  &         0.019        &     2.68e-06    &     2.99e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")}              &       0.0008  &        0.000     &     2.433  &         0.015        &        0.000    &        0.001     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}                                                     &      -8.3636  &        2.346     &    -3.565  &         0.000        &      -12.978    &       -3.749     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Atk\")}                                   &     850.5436  &      385.064     &     2.209  &         0.028        &       93.112    &     1607.975     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Atk\")}                                              &       0.1388  &        0.040     &     3.500  &         0.001        &        0.061    &        0.217     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Atk\")}                            &       2.1809  &        1.136     &     1.920  &         0.056        &       -0.054    &        4.416     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Atk\")}                                             &       0.0831  &        0.038     &     2.162  &         0.031        &        0.007    &        0.159     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Atk\")}                           &      -7.3121  &        3.376     &    -2.166  &         0.031        &      -13.953    &       -0.671     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Atk\")}                                      &      -0.0014  &        0.001     &    -2.480  &         0.014        &       -0.003    &       -0.000     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")}                    &      -0.0434  &        0.022     &    -2.010  &         0.045        &       -0.086    &       -0.001     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Atk\")}                                               &       0.1011  &        0.035     &     2.872  &         0.004        &        0.032    &        0.170     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                             &     -12.6343  &        5.613     &    -2.251  &         0.025        &      -23.674    &       -1.594     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Atk\")}                                        &      -0.0018  &        0.001     &    -3.102  &         0.002        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                      &       0.0151  &        0.009     &     1.609  &         0.109        &       -0.003    &        0.034     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Atk\")}                                       &      -0.0012  &        0.001     &    -1.860  &         0.064        &       -0.002    &     6.62e-05     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                     &       0.1210  &        0.054     &     2.260  &         0.024        &        0.016    &        0.226     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Atk\")}                                &    2.125e-05  &      9.1e-06     &     2.334  &         0.020        &     3.34e-06    &     3.92e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}              &    6.438e-06  &     7.69e-05     &     0.084  &         0.933        &       -0.000    &        0.000     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                        &       0.1265  &        0.033     &     3.821  &         0.000        &        0.061    &        0.192     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                      &      -5.0544  &        2.506     &    -2.017  &         0.044        &       -9.983    &       -0.126     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                 &      -0.0021  &        0.001     &    -3.606  &         0.000        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}               &      -0.0346  &        0.017     &    -1.992  &         0.047        &       -0.069    &       -0.000     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                &      -0.0012  &        0.000     &    -2.406  &         0.017        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}              &       0.0446  &        0.025     &     1.794  &         0.074        &       -0.004    &        0.093     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                         &    1.973e-05  &     7.28e-06     &     2.710  &         0.007        &     5.41e-06    &      3.4e-05     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}       &       0.0005  &        0.000     &     1.957  &         0.051        &    -2.56e-06    &        0.001     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                  &      -0.0013  &        0.000     &    -2.740  &         0.006        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                &       0.0841  &        0.040     &     2.125  &         0.034        &        0.006    &        0.162     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                           &    2.379e-05  &     7.85e-06     &     3.030  &         0.003        &     8.34e-06    &     3.92e-05     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}         &    2.864e-05  &     7.73e-05     &     0.370  &         0.711        &       -0.000    &        0.000     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                          &    1.284e-05  &     7.46e-06     &     1.721  &         0.086        &    -1.83e-06    &     2.75e-05     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}        &      -0.0008  &        0.000     &    -2.085  &         0.038        &       -0.002    &    -4.68e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                   &    -2.53e-07  &      1.1e-07     &    -2.292  &         0.023        &     -4.7e-07    &    -3.59e-08     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")} &   -1.425e-06  &     1.14e-06     &    -1.249  &         0.212        &    -3.67e-06    &     8.19e-07     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.664  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.20e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.2e+16. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.467\n",
       "Model:                            OLS   Adj. R-squared:                  0.369\n",
       "Method:                 Least Squares   F-statistic:                     4.764\n",
       "Date:                Thu, 14 Nov 2024   Prob (F-statistic):           4.23e-21\n",
       "Time:                        21:13:28   Log-Likelihood:                -1738.6\n",
       "No. Observations:                 400   AIC:                             3603.\n",
       "Df Residuals:                     337   BIC:                             3855.\n",
       "Df Model:                          62                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================================================================\n",
       "                                                                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                                          521.5715    130.273      4.004      0.000     265.322     777.821\n",
       "Legendary[T.True]                                                   -6.1179      2.846     -2.150      0.032     -11.716      -0.520\n",
       "Attack                                                              -8.1938      2.329     -3.518      0.000     -12.775      -3.612\n",
       "Attack:Legendary[T.True]                                         -1224.9610    545.105     -2.247      0.025   -2297.199    -152.723\n",
       "Defense                                                             -6.1989      2.174     -2.851      0.005     -10.475      -1.923\n",
       "Defense:Legendary[T.True]                                         -102.4030     96.565     -1.060      0.290    -292.350      87.544\n",
       "Attack:Defense                                                       0.0985      0.033      2.982      0.003       0.034       0.164\n",
       "Attack:Defense:Legendary[T.True]                                    14.6361      6.267      2.336      0.020       2.310      26.963\n",
       "Speed                                                               -7.2261      2.178     -3.318      0.001     -11.511      -2.942\n",
       "Speed:Legendary[T.True]                                            704.8798    337.855      2.086      0.038      40.309    1369.450\n",
       "Attack:Speed                                                         0.1264      0.038      3.351      0.001       0.052       0.201\n",
       "Attack:Speed:Legendary[T.True]                                       5.8648      2.692      2.179      0.030       0.570      11.160\n",
       "Defense:Speed                                                        0.1026      0.039      2.634      0.009       0.026       0.179\n",
       "Defense:Speed:Legendary[T.True]                                     -6.9266      3.465     -1.999      0.046     -13.742      -0.111\n",
       "Attack:Defense:Speed                                                -0.0016      0.001     -2.837      0.005      -0.003      -0.001\n",
       "Attack:Defense:Speed:Legendary[T.True]                              -0.0743      0.030     -2.477      0.014      -0.133      -0.015\n",
       "Q(\"Sp. Def\")                                                        -5.3982      1.938     -2.785      0.006      -9.211      -1.586\n",
       "Legendary[T.True]:Q(\"Sp. Def\")                                    -282.2496    126.835     -2.225      0.027    -531.738     -32.761\n",
       "Attack:Q(\"Sp. Def\")                                                  0.1094      0.034      3.233      0.001       0.043       0.176\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Def\")                               12.6503      5.851      2.162      0.031       1.141      24.160\n",
       "Defense:Q(\"Sp. Def\")                                                 0.0628      0.028      2.247      0.025       0.008       0.118\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Def\")                               3.3949      1.783      1.904      0.058      -0.112       6.902\n",
       "Attack:Defense:Q(\"Sp. Def\")                                         -0.0012      0.000     -2.730      0.007      -0.002      -0.000\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")                       -0.1456      0.065     -2.253      0.025      -0.273      -0.018\n",
       "Speed:Q(\"Sp. Def\")                                                   0.0624      0.031      2.027      0.043       0.002       0.123\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Def\")                                -3.2219      1.983     -1.625      0.105      -7.122       0.678\n",
       "Attack:Speed:Q(\"Sp. Def\")                                           -0.0014      0.001     -2.732      0.007      -0.002      -0.000\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")                         -0.0695      0.033     -2.100      0.036      -0.135      -0.004\n",
       "Defense:Speed:Q(\"Sp. Def\")                                          -0.0008      0.000     -1.743      0.082      -0.002       0.000\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")                         0.0334      0.021      1.569      0.117      -0.008       0.075\n",
       "Attack:Defense:Speed:Q(\"Sp. Def\")                                 1.629e-05   6.92e-06      2.355      0.019    2.68e-06    2.99e-05\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")                  0.0008      0.000      2.433      0.015       0.000       0.001\n",
       "Q(\"Sp. Atk\")                                                        -8.3636      2.346     -3.565      0.000     -12.978      -3.749\n",
       "Legendary[T.True]:Q(\"Sp. Atk\")                                     850.5436    385.064      2.209      0.028      93.112    1607.975\n",
       "Attack:Q(\"Sp. Atk\")                                                  0.1388      0.040      3.500      0.001       0.061       0.217\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Atk\")                                2.1809      1.136      1.920      0.056      -0.054       4.416\n",
       "Defense:Q(\"Sp. Atk\")                                                 0.0831      0.038      2.162      0.031       0.007       0.159\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Atk\")                              -7.3121      3.376     -2.166      0.031     -13.953      -0.671\n",
       "Attack:Defense:Q(\"Sp. Atk\")                                         -0.0014      0.001     -2.480      0.014      -0.003      -0.000\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")                       -0.0434      0.022     -2.010      0.045      -0.086      -0.001\n",
       "Speed:Q(\"Sp. Atk\")                                                   0.1011      0.035      2.872      0.004       0.032       0.170\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Atk\")                               -12.6343      5.613     -2.251      0.025     -23.674      -1.594\n",
       "Attack:Speed:Q(\"Sp. Atk\")                                           -0.0018      0.001     -3.102      0.002      -0.003      -0.001\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")                          0.0151      0.009      1.609      0.109      -0.003       0.034\n",
       "Defense:Speed:Q(\"Sp. Atk\")                                          -0.0012      0.001     -1.860      0.064      -0.002    6.62e-05\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")                         0.1210      0.054      2.260      0.024       0.016       0.226\n",
       "Attack:Defense:Speed:Q(\"Sp. Atk\")                                 2.125e-05    9.1e-06      2.334      0.020    3.34e-06    3.92e-05\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")               6.438e-06   7.69e-05      0.084      0.933      -0.000       0.000\n",
       "Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                            0.1265      0.033      3.821      0.000       0.061       0.192\n",
       "Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                         -5.0544      2.506     -2.017      0.044      -9.983      -0.126\n",
       "Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                    -0.0021      0.001     -3.606      0.000      -0.003      -0.001\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  -0.0346      0.017     -1.992      0.047      -0.069      -0.000\n",
       "Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                   -0.0012      0.000     -2.406      0.017      -0.002      -0.000\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  0.0446      0.025      1.794      0.074      -0.004       0.093\n",
       "Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                          1.973e-05   7.28e-06      2.710      0.007    5.41e-06     3.4e-05\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           0.0005      0.000      1.957      0.051   -2.56e-06       0.001\n",
       "Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                     -0.0013      0.000     -2.740      0.006      -0.002      -0.000\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                    0.0841      0.040      2.125      0.034       0.006       0.162\n",
       "Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                            2.379e-05   7.85e-06      3.030      0.003    8.34e-06    3.92e-05\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")          2.864e-05   7.73e-05      0.370      0.711      -0.000       0.000\n",
       "Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                           1.284e-05   7.46e-06      1.721      0.086   -1.83e-06    2.75e-05\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           -0.0008      0.000     -2.085      0.038      -0.002   -4.68e-05\n",
       "Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                    -2.53e-07    1.1e-07     -2.292      0.023    -4.7e-07   -3.59e-08\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\") -1.425e-06   1.14e-06     -1.249      0.212   -3.67e-06    8.19e-07\n",
       "==============================================================================\n",
       "Omnibus:                      214.307   Durbin-Watson:                   1.992\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2354.664\n",
       "Skew:                           2.026   Prob(JB):                         0.00\n",
       "Kurtosis:                      14.174   Cond. No.                     1.20e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.2e+16. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary'\n",
    "model4_linear_form += ' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "# DO NOT try adding '* C(Generation) * C(Q(\"Type 1\")) * C(Q(\"Type 2\"))'\n",
    "# That's 6*18*19 = 6*18*19 possible interaction combinations...\n",
    "# ...a huge number that will blow up your computer\n",
    "\n",
    "model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)\n",
    "model4_fit = model4_spec.fit()\n",
    "model4_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ccc4825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.46709442115833855\n",
      "'Out of sample' R-squared: 0.002485342598992873\n"
     ]
    }
   ],
   "source": [
    "yhat_model4 = model4_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model4_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model4)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97fda2",
   "metadata": {},
   "source": [
    "1st cell: introduced libraries, prepares the dataset for evaluating model generalizability by splitting it into pokeaman_train and pokeaman_test. Using separate datasets for training and testing allows us to examine how well the model performs on new data versus the data it was trained on. The aim is to detect potential overfitting.\n",
    "\n",
    "2nd cell: Builds a simple linear regression model with HP as the dependent variable, predicted by Attack and Defense, and model's summary will show us initial insights, like coefficients and the \"in-sample\" R-squared, representing the proportion of variation in HP that this simpler model explains on the training data\n",
    "\n",
    "3rd cell: calculates the \"in-sample\" R-squared for the baseline model using the training data. This R-squared measures how well HP is explained by the model within the same dataset used to train it, which is often relatively high. Then calculates the \"out-of-sample\" R-squared on the testing data, giving insight into how the model generalizes to new data. If this R-squared is substantially lower than the in-sample R-squared, it would suggest overfitting, as the model performs poorly outside its training context.\n",
    "\n",
    "4th cell: introduces a more complex model with several interactions to capture relationships among additional predictors like Speed, Legendary, and special attributes (Sp. Def and Sp. Atk). By increasing complexity, the model may improve its \"in-sample\" fit on pokeaman_train, potentially achieving a higher R-squared, as it accounts for more variance within the training data.\n",
    "\n",
    "5th cell: evaluates the more complex model similarly to Cell 3, calculating both the \"in-sample\" and \"out-of-sample\" R-squared values. If the model performs comparably well on both training and testing data, it’s likely to generalize better. A significant drop in \"out-of-sample\" R-squared relative to \"in-sample\" R-squared would indicate overfitting, suggesting that the additional complexity made the model more specialized to the training data, reducing its effectiveness on new data.\n",
    "\n",
    "These cells illustrate the process of model evaluation for generalizability by contrasting \"in-sample\" and \"out-of-sample\" R-squared metrics. They demonstrate how adding complexity (from baseline to complex model) can improve in-sample fit but may lead to overfitting, highlighted by a potential decrease in out-of-sample R-squared. This workflow is foundational in machine learning to ensure that the model not only fits the data it was trained on but also performs reliably on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e870e5",
   "metadata": {},
   "source": [
    "# 6. Work with a ChatBot to understand how the model4_linear_form (linear form specification of model4) creates new predictor variables as the columns of the so-called \"design matrix\" model4_spec.exog (model4_spec.exog.shape) used to predict the outcome variable model4_spec.endog and why the so-called multicollinearity in this \"design matrix\" (observed in np.corrcoef(model4_spec.exog)) contribues to the lack of \"out of sample\" generalization of predictions from model4_fit; then, explain this consisely in your own works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79fbb7c",
   "metadata": {},
   "source": [
    "This question focuses on understanding model complexity, multicollinearity, and generalizability by examining the design matrix (model4_spec.exog) used in Model 4, particularly through the concept of the condition number and centering/scaling of predictor variables.\n",
    "\n",
    "The complexity of Model 4 introduces a large design matrix with highly correlated predictor terms, leading to multicollinearity. This multicollinearity, signaled by an extremely high condition number, undermines the model's stability and generalizability. As a result, Model 4 fits noise in the training data rather than true patterns, causing it to perform poorly on new data. Centering and scaling help reduce some numerical artifacts but don’t fully resolve the multicollinearity. In contrast, simpler models like Model 3 avoid overfitting by focusing on stronger, more reliable associations that generalize well across datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1970e856",
   "metadata": {},
   "source": [
    "# 7. Discuss with a ChatBot the rationale and principles by which model5_linear_form is extended and developed from model3_fit and model4_fit; model6_linear_form is extended and developed from model5_linear_form; and model7_linear_form is extended and developed from model6_linear_form; then, explain this breifly and consisely in your own words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d08cfda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3920134083531893\n",
      "'Out of sample' R-squared: 0.30015614488652215\n",
      "'In sample' R-squared:     0.3326310334310908\n",
      "'Out of sample' R-squared: 0.29572460427079933\n",
      "'In sample' R-squared:     0.37818209127432456\n",
      "'Out of sample' R-squared: 0.35055389205977444\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>2.34e+09</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n",
       "\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } & 2.34e+09  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's something a little more reasonable...\n",
    "model5_linear_form = 'HP ~ Attack + Defense + Speed + Legendary'\n",
    "model5_linear_form += ' + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "model5_linear_form += ' + C(Generation) + C(Q(\"Type 1\")) + C(Q(\"Type 2\"))'\n",
    "\n",
    "model5_spec = smf.ols(formula=model5_linear_form, data=pokeaman_train)\n",
    "model5_fit = model5_spec.fit()\n",
    "model5_fit.summary()\n",
    "yhat_model5 = model5_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model5_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model5)[0,1]**2)\n",
    "# Here's something a little more reasonable...\n",
    "model6_linear_form = 'HP ~ Attack + Speed + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "# And here we'll add the significant indicators from the previous model\n",
    "# https://chatgpt.com/share/81ab88df-4f07-49f9-a44a-de0cfd89c67c\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model6_linear_form += ' + I(Generation==2)'\n",
    "model6_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model6_spec = smf.ols(formula=model6_linear_form, data=pokeaman_train)\n",
    "model6_fit = model6_spec.fit()\n",
    "model6_fit.summary()\n",
    "yhat_model6 = model6_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2)\n",
    "# And here's a slight change that seems to perhaps improve prediction...\n",
    "model7_linear_form = 'HP ~ Attack * Speed * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form += ' + I(Generation==2)'\n",
    "model7_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model7_spec = smf.ols(formula=model7_linear_form, data=pokeaman_train)\n",
    "model7_fit = model7_spec.fit()\n",
    "model7_fit.summary()\n",
    "yhat_model7 = model7_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2)\n",
    "# And here's a slight change that seems to perhas improve prediction...\n",
    "model7_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Speed))'\n",
    "model7_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# We DO NOT center and scale indicator variables\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form_CS += ' + I(Generation==2)'\n",
    "model7_linear_form_CS += ' + I(Generation==5)'\n",
    "\n",
    "model7_CS_spec = smf.ols(formula=model7_linear_form_CS, data=pokeaman_train)\n",
    "model7_CS_fit = model7_CS_spec.fit()\n",
    "model7_CS_fit.summary().tables[-1] \n",
    "# \"Cond. No.\" is NOW 15.4 due to centering and scaling\n",
    "# \"Cond. No.\" WAS 2,340,000,000 WITHOUT to centering and scaling\n",
    "model7_fit.summary().tables[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210ab3df",
   "metadata": {},
   "source": [
    "### The rationale and principles by which model5_linear_form is extended and developed from model3_fit and model4_fit\n",
    "\n",
    "The rationale for how model5_linear_form is extended and developed from model3_fit and model4_fit is on the baseline model (Model 3) by incorporating additional relevant predictors (Speed, Sp. Def, Sp. Atk) and categorical variables (Generation, Type 1, Type 2), similar to Model 4. This extension purpose is to improve predictive accuracy without overwhelming complexity. As for the principles for the extension and development, model 5 leverages a balanced expansion approach, adding predictors that contribute meaningful variation without creating excessive multicollinearity or overfitting. It combines key variables from the simpler Model 3 and more complex Model 4 in a controlled way to enhance generalizability.\n",
    "\n",
    "### model6_linear_form is extended and developed from model5_linear_form\n",
    "\n",
    "The rationale behind the extension and development from model5_linear_form is that model 6 tries to simplify model 5 by including only the most significant predictors from Model 5, such as certain types and generations, while excluding predictors that showed less relevance. The principle is that this model prioritizes parsimony (favouring a simpler model with fewer parameters over more complex models as long as the models fit the data similarly well), refining the model to include only statistically significant terms. By doing so, Model 6 improves fit without the risk of multicollinearity, aiming to capture core predictive relationships with minimal complexity. \n",
    "\n",
    "### model7_linear_form is extended and developed from model6_linear_form\n",
    "\n",
    "The rationale behind the extension and development from model6_linear_form to model 7 is that model 7 builds on model 6 by reintroducing the interaction terms between core variables (Attack, Speed, Sp. Def, Sp. Atk) while keeping key categorical indicators. This model tests for any non-linear relationships among predictors that might improve predictive power. The principle is that this model is guided by controlled complexity with interaction terms, and it strategically adds complexity where evidence suggests it might improve prediction, yet without leading to severe multicollinearity (condition number remains reasonable after centering and scaling).\n",
    "\n",
    "With each new model, it aims to improve the generalizability and predicitive accuracy by adding complexity specifically in areas where it is supported by evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b408170",
   "metadata": {},
   "source": [
    "# 8. Work with a ChatBot to write a for loop to create, collect, and visualize many different paired \"in sample\" and \"out of sample\" model performance metric actualizations (by not using np.random.seed(130) within each loop iteration); and explain in your own words the meaning of your results and purpose of this demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a9c99fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set parameters\n",
    "reps = 100\n",
    "in_sample_Rsquared = np.zeros(reps)\n",
    "out_of_sample_Rsquared = np.zeros(reps)\n",
    "\n",
    "# Specify the model formula (e.g., from model3)\n",
    "linear_form = 'HP ~ Attack + Defense'\n",
    "\n",
    "for i in range(reps):\n",
    "    # Create a random 50-50 train-test split each iteration\n",
    "    pokeaman_train, pokeaman_test = train_test_split(pokeaman, train_size=0.5)\n",
    "    \n",
    "    # Fit the model\n",
    "    final_model_fit = smf.ols(formula=linear_form, data=pokeaman_train).fit()\n",
    "    \n",
    "    # Calculate and store \"in-sample\" R-squared\n",
    "    in_sample_Rsquared[i] = final_model_fit.rsquared\n",
    "    \n",
    "    # Calculate and store \"out-of-sample\" R-squared\n",
    "    y_test = pokeaman_test.HP\n",
    "    yhat_test = final_model_fit.predict(pokeaman_test)\n",
    "    out_of_sample_Rsquared[i] = np.corrcoef(y_test, yhat_test)[0, 1] ** 2\n",
    "\n",
    "# Convert results to a DataFrame for visualization\n",
    "df = pd.DataFrame({\n",
    "    \"In Sample Performance (R-squared)\": in_sample_Rsquared,\n",
    "    \"Out of Sample Performance (R-squared)\": out_of_sample_Rsquared\n",
    "})\n",
    "\n",
    "# Create the scatter plot with Plotly\n",
    "fig = px.scatter(df, x=\"In Sample Performance (R-squared)\", y=\"Out of Sample Performance (R-squared)\",\n",
    "                 title=\"In-sample vs Out-of-sample R-squared across Random Splits\")\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode=\"lines\", name=\"y=x\", line=dict(dash=\"dash\")))\n",
    "fig.update_layout(xaxis_title=\"In-sample R-squared\", yaxis_title=\"Out-of-sample R-squared\")\n",
    "fig.show(renderer='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e20d52",
   "metadata": {},
   "source": [
    "# 9. Work with a ChatBot to understand the meaning of the illustration below; and, explain this in your own words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "954d0a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.37818209127432456 (original)\n",
      "'Out of sample' R-squared: 0.00018905947734540018 (original)\n",
      "'In sample' R-squared:     0.5726118179916575 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.11151363354803218 (gen1_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model7_gen1_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model7_gen1_predict_future_fit = model7_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model7_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "834ec9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.37818209127432456 (original)\n",
      "'Out of sample' R-squared: 0.00018905947734540018 (original)\n",
      "'In sample' R-squared:     0.3904756578094535 (gen1to5_predict_future)\n",
      "'Out of sample' R-squared: 0.23394915464343125 (gen1to5_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model7_gen1to5_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model7_gen1to5_predict_future_fit = model7_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model7_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3e903d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3326310334310908 (original)\n",
      "'Out of sample' R-squared: 0.0003901627672308394 (original)\n",
      "'In sample' R-squared:     0.4433880517727282 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.1932858534276128 (gen1_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model6_gen1_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model6_gen1_predict_future_fit = model6_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model6_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "392fbea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3326310334310908 (original)\n",
      "'Out of sample' R-squared: 0.0003901627672308394 (original)\n",
      "'In sample' R-squared:     0.33517279824114776 (gen1to5_predict_future)\n",
      "'Out of sample' R-squared: 0.26262690178799936 (gen1to5_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model6_gen1to5_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model6_gen1to5_predict_future_fit = model6_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model6_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e449ac4",
   "metadata": {},
   "source": [
    "### code explanation\n",
    "\n",
    "Overall, the code above illustrates the concept of temporal generalization and model performance across generations in the Pokémon dataset by analyzing how well models trained on early generations (e.g., Generation 1 or Generations 1–5) generalize to future generations (e.g., Generation 6).\n",
    "\n",
    "In the first step of the code, the code defines and fits model 7 and 6 on specific subsets (Generation 1 only tests how a model trained on the oldest generation performs on future generations)(Generations 1–5 test how a model trained on all but the latest generation (Generation 6) perform on unseen, newer data). Then, it prints the \"in-sample\" and \"out-of-sample\" R-squared values for the original model (e.g., model7_fit and model6_fit) on the full training and testing datasets, and these values give a baseline performance measure when the model is trained on a representative sample of all generations. Then, in the next step, it calculates \"In-Sample\" R-Squared on the subset data which indicates how well the model fits the specific subset of generations it was trained on. Afterwards, it calculates \"Out-of-Sample\" R-Squared on future generations not included in the training data (Generations 2–6 for the first model and Generation 6 for the second). This measures how well the subset-trained models generalize to new generations. Therefore, this code tests the temporal generalizability of models by training them on older data and evaluating their predictive power on newer generations. It helps reveal if models can maintain performance across different time periods, indicating the strength of the model’s predictive associations over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf00f0",
   "metadata": {},
   "source": [
    "### chat summary (Q5-Q9):\n",
    "### chat link: https://chatgpt.com/share/67366394-14a0-8003-a2ba-15444d4eeb5d\n",
    "\n",
    "In this chat, we analyzed a series of increasingly complex models (`model3`, `model4`, `model5`, `model6`, and `model7`) used to predict Pokémon characteristics, specifically focusing on understanding the principles of model building, generalizability, and multicollinearity. We started by exploring the basics of \"in-sample\" and \"out-of-sample\" R-squared metrics, noting how these metrics highlight overfitting when the \"in-sample\" R-squared is much higher than the \"out-of-sample\" R-squared. Next, we delved into multicollinearity within design matrices, using the condition number as a diagnostic tool to assess its impact on model performance, particularly for `model4`, which exhibited severe multicollinearity and poor generalizability.\n",
    "\n",
    "From there, we discussed the incremental development of models: how `model5` builds on `model3` and `model4` by balancing complexity and simplicity, how `model6` refines `model5` by adding statistically significant predictors, and how `model7` extends `model6` by adding selected interactions to capture more nuanced relationships. We then implemented a loop to repeatedly fit models across random train-test splits, collecting \"in-sample\" and \"out-of-sample\" R-squared values to assess model stability across different data splits, revealing how variability in sampling affects performance metrics and can influence perceptions of model robustness.\n",
    "\n",
    "Finally, we examined temporal generalizability by training `model6` and `model7` on specific subsets of Pokémon generations (e.g., only Generation 1 or Generations 1–5) and testing their predictions on later generations. This exercise demonstrated the challenges of generalizing across time when trained on limited data and underscored the importance of using diverse data for models intended to predict across varied or evolving datasets. Throughout, we emphasized balancing complexity with parsimony and using statistical diagnostics to guide model improvement, aiming to achieve both accurate and generalizable models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb98651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
